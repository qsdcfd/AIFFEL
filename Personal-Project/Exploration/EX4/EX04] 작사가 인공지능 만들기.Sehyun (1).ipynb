{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ethical-alberta",
   "metadata": {},
   "source": [
    "# 작사가 인공지능\n",
    "\n",
    "오늘 하는 프로젝트는 NLP기반인 인공지능입니다.\n",
    "\n",
    "직접 인공지능이 많은 데이터를 기반으로 어떻게 언어를 표현하고 있는 그 과정을 수행하는 프로젝트입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-fiber",
   "metadata": {},
   "source": [
    "## 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "saving-central",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/aiffel/aiffel/lyricists/data/data': Read-only file system\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p ~/aiffel/lyricists/models\n",
    "! ln -s ~/data ~/aiffel/lyricists/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-increase",
   "metadata": {},
   "source": [
    "## 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "neither-procedure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [' There must be some kind of way outta here', 'Said the joker to the thief', \"There's too much confusion\"]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricists/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-growth",
   "metadata": {},
   "source": [
    "우리는 문장데이터를 분석할 때 원하는 것만 제공이 되면 좋겠지만 실제로는 그렇지 못하기 때문에 정제나 전처리 과정을 해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "opened-invitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There must be some kind of way outta here\n",
      "Said the joker to the thief\n",
      "There's too much confusion\n",
      "I can't get no relief Business men, they drink my wine\n",
      "Plowman dig my earth\n",
      "None were level on the mind\n",
      "Nobody up at his word\n",
      "Hey, hey No reason to get excited\n",
      "The thief he kindly spoke\n",
      "There are many here among us\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue    \n",
    "    if sentence[-1] == \":\": continue   \n",
    "\n",
    "    if idx > 9: break   \n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-sunset",
   "metadata": {},
   "source": [
    "평가문항2번"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-heading",
   "metadata": {},
   "source": [
    "## 데이터 정제\n",
    "\n",
    "이제 단계별로 수행할 것입니다.\n",
    "\n",
    "1.모두 소문자화.\n",
    "\n",
    "2.공백 지우기\n",
    "\n",
    "3.공백 추가(특수문자 양쪽)\n",
    "\n",
    "4.공백이 많으면 하나로 통일\n",
    "\n",
    "5.\"\"외에 다 공백으로 바꿈\n",
    "\n",
    "6.토큰 갯수가 13개 이상이 아니라면\n",
    "문장의 시작과 끝에 start와 end추가\n",
    "\n",
    "이상이면 0\n",
    "\n",
    "7.아무 문장 넣어서 실행해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "olympic-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence1 = sentence.split(' ')\n",
    "    \n",
    "    if len(sentence1) >13:\n",
    "        return 0\n",
    "    else:\n",
    "        sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "        return sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-defensive",
   "metadata": {},
   "source": [
    "위의 코드를 실제로 진행하고 함수 preprocess_setence에서 만든 \n",
    "문장의 전처리 과정을 지난 후 저장을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "legislative-financing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> there must be some kind of way outta here <end>',\n",
       " '<start> said the joker to the thief <end>',\n",
       " '<start> there s too much confusion <end>',\n",
       " '<start> i can t get no relief business men , they drink my wine <end>',\n",
       " '<start> plowman dig my earth <end>',\n",
       " '<start> none were level on the mind <end>',\n",
       " '<start> nobody up at his word <end>',\n",
       " '<start> hey , hey no reason to get excited <end>',\n",
       " '<start> the thief he kindly spoke <end>',\n",
       " '<start> there are many here among us <end>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    if preprocessed_sentence ==0:\n",
    "        pass\n",
    "    else:\n",
    "        corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-macintosh",
   "metadata": {},
   "source": [
    "데이터 전처리 과정은 끝났으나 단어와 숫자를 매칭시켜주는 토큰화 작업을 해야합니다. 왜냐하면 컴퓨터는 언어가 아닌 숫자로 인식을 하는 스타일이기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "nuclear-annual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  62 271 ...   0   0   0]\n",
      " [  2 118   6 ...   0   0   0]\n",
      " [  2  62  17 ...   0   0   0]\n",
      " ...\n",
      " [  2  75  45 ...   3   0   0]\n",
      " [  2  49   5 ...   0   0   0]\n",
      " [  2  13 633 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f33cf870b10>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def tokenize(corpus):\n",
    "   \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "   \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-compensation",
   "metadata": {},
   "source": [
    "위의 과정이 진짜로 변했나 확인 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "therapeutic-message",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-criminal",
   "metadata": {},
   "source": [
    "참고로 LSTM에서는 많은 데이터로 많은 답을 얻기 때문에\n",
    "\n",
    "train은 첫 문장 ~ 끝에 1개 뺀 문장 구성\n",
    "\n",
    "trget: 첫번째 단어  뺀 문장으로 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "premier-communications",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  62 271  27  94 546  20  86 742  90   3   0   0   0]\n",
      "[ 62 271  27  94 546  20  86 742  90   3   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "src_input = tensor[:, :-1]  \n",
    "tgt_input = tensor[:, 1:]    \n",
    "print(src_input[0])\n",
    "print(tgt_input[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-plate",
   "metadata": {},
   "source": [
    "데이터 셋 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "electric-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-insured",
   "metadata": {},
   "source": [
    "## 평가 데이터 셋 분리\n",
    "\n",
    "앞서 알려준 사이킷런 패키지를 통해서 8:2로 train과 target를\n",
    "train과 validation셋으로 분리를 합니다(CS231n 2강파트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "civic-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          shuffle=True, \n",
    "                                                          random_state=34)\n",
    "                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "thick-mechanics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train:  (124810, 14)\n",
      "Target Train:  (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Source Train: ', enc_train.shape)\n",
    "print('Target Train: ', dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-pasta",
   "metadata": {},
   "source": [
    "## 평가문항 3번"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "opposite-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "running-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "embedding_size = 19\n",
    "hidden_size = 2048\n",
    "\n",
    "\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "history = []\n",
    "epochs = 10\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "removed-warrant",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-1.71207921e-05,  2.04618646e-05, -2.66351435e-05, ...,\n",
       "          5.77007377e-05,  3.66221866e-05, -6.11733640e-06],\n",
       "        [ 1.83372867e-05,  4.45840888e-05, -1.13975666e-05, ...,\n",
       "          8.94761906e-05,  4.75149754e-05, -5.91201533e-05],\n",
       "        [ 1.53110086e-05,  7.79168258e-05, -4.83180047e-05, ...,\n",
       "          6.97352589e-05,  4.63235701e-05, -5.45579496e-05],\n",
       "        ...,\n",
       "        [-1.80235904e-04,  6.62449602e-05,  2.33214392e-04, ...,\n",
       "         -9.53052004e-05,  4.37235140e-04,  8.65938346e-05],\n",
       "        [-2.65075039e-04,  5.50332406e-05,  3.14246456e-04, ...,\n",
       "         -4.37559320e-05,  5.06745651e-04,  1.45873753e-04],\n",
       "        [-3.40410508e-04,  4.20210854e-05,  3.88511515e-04, ...,\n",
       "          5.05564367e-06,  5.78024832e-04,  2.02504729e-04]],\n",
       "\n",
       "       [[-1.71207921e-05,  2.04618646e-05, -2.66351435e-05, ...,\n",
       "          5.77007377e-05,  3.66221866e-05, -6.11733640e-06],\n",
       "        [ 2.99546446e-05,  1.26627617e-06, -5.22973496e-05, ...,\n",
       "         -3.72258171e-07, -6.72835858e-06, -7.58091119e-05],\n",
       "        [ 5.77750470e-05, -8.87060742e-05, -4.29365209e-05, ...,\n",
       "          6.51225200e-05, -8.71971642e-05, -1.13244852e-04],\n",
       "        ...,\n",
       "        [-2.61137116e-04,  5.61879315e-05,  3.01632157e-04, ...,\n",
       "          9.19997910e-05,  1.51029526e-04,  1.62664743e-04],\n",
       "        [-3.27598595e-04,  2.39190595e-05,  3.94042232e-04, ...,\n",
       "          1.30947577e-04,  3.01795255e-04,  1.96287845e-04],\n",
       "        [-3.87183391e-04, -3.75718787e-06,  4.75023902e-04, ...,\n",
       "          1.64793557e-04,  4.40112170e-04,  2.26554461e-04]],\n",
       "\n",
       "       [[-1.71207921e-05,  2.04618646e-05, -2.66351435e-05, ...,\n",
       "          5.77007377e-05,  3.66221866e-05, -6.11733640e-06],\n",
       "        [-4.89221784e-05,  2.95838181e-05,  9.85971565e-06, ...,\n",
       "          7.16325521e-05,  6.94953196e-05, -3.57826902e-05],\n",
       "        [-1.35026730e-05, -4.29069723e-06,  7.30563333e-05, ...,\n",
       "          2.92891691e-05,  2.58174659e-05, -2.43533250e-05],\n",
       "        ...,\n",
       "        [-8.67713970e-05,  1.06021667e-04, -1.19569209e-04, ...,\n",
       "          9.54516363e-05, -4.54795445e-05,  1.75916051e-04],\n",
       "        [-1.43122263e-04,  7.53566346e-05, -2.81285920e-05, ...,\n",
       "          9.21548053e-05,  1.99139486e-05,  1.78022616e-04],\n",
       "        [-2.13756866e-04,  4.42066994e-05,  8.24203889e-05, ...,\n",
       "          1.04229097e-04,  1.10963061e-04,  1.93074564e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.71207921e-05,  2.04618646e-05, -2.66351435e-05, ...,\n",
       "          5.77007377e-05,  3.66221866e-05, -6.11733640e-06],\n",
       "        [ 1.19034139e-05,  6.30591021e-05, -3.81393656e-05, ...,\n",
       "          6.25311295e-05,  2.14803313e-05, -5.90626623e-05],\n",
       "        [ 3.72749528e-05,  5.40954852e-06, -1.13002192e-04, ...,\n",
       "          3.85646126e-05,  4.89399099e-06, -5.00414317e-05],\n",
       "        ...,\n",
       "        [ 5.70393968e-05,  4.21903445e-04,  7.60217881e-05, ...,\n",
       "          1.76560628e-04, -4.45807527e-05,  7.58936876e-05],\n",
       "        [ 1.44018495e-05,  3.73311079e-04,  1.74199391e-04, ...,\n",
       "          1.77012160e-04,  1.20994091e-05,  8.20603891e-05],\n",
       "        [-4.00491117e-05,  3.12594668e-04,  2.77120271e-04, ...,\n",
       "          1.85045370e-04,  1.02623155e-04,  1.01094054e-04]],\n",
       "\n",
       "       [[-1.71207921e-05,  2.04618646e-05, -2.66351435e-05, ...,\n",
       "          5.77007377e-05,  3.66221866e-05, -6.11733640e-06],\n",
       "        [ 3.12455522e-05,  1.94309359e-05,  2.04234243e-06, ...,\n",
       "          1.19059827e-04,  7.33130437e-05, -5.63993526e-05],\n",
       "        [ 4.71106905e-05,  9.40354294e-05,  5.73577054e-05, ...,\n",
       "          1.92983644e-04,  8.25356619e-05, -6.95654380e-05],\n",
       "        ...,\n",
       "        [ 7.03187397e-05,  1.62984128e-04, -9.53014533e-05, ...,\n",
       "          1.50706881e-04,  1.37368625e-04, -6.91323003e-05],\n",
       "        [ 5.94242883e-05,  1.02991529e-04, -7.29520398e-05, ...,\n",
       "          1.36555478e-04,  1.59296673e-04, -9.05929555e-05],\n",
       "        [ 5.59104883e-05,  1.74943008e-04, -9.47668523e-05, ...,\n",
       "          4.64522491e-05,  1.64992074e-04, -5.51699559e-05]],\n",
       "\n",
       "       [[-1.71207921e-05,  2.04618646e-05, -2.66351435e-05, ...,\n",
       "          5.77007377e-05,  3.66221866e-05, -6.11733640e-06],\n",
       "        [ 2.98407758e-05,  2.01453668e-06, -4.55407717e-05, ...,\n",
       "          2.66446423e-05, -3.71657080e-07, -3.17899139e-05],\n",
       "        [ 9.62842605e-05, -2.65397121e-05, -2.70911187e-05, ...,\n",
       "          2.85560382e-05,  3.53209398e-05, -4.42813252e-05],\n",
       "        ...,\n",
       "        [-1.64173602e-04,  2.23687559e-04,  2.87677656e-04, ...,\n",
       "          2.55963707e-04,  2.08410624e-04,  3.10243893e-04],\n",
       "        [-2.55027669e-04,  1.73079548e-04,  3.83826846e-04, ...,\n",
       "          2.79408065e-04,  3.48481262e-04,  3.17232480e-04],\n",
       "        [-3.37174250e-04,  1.27602936e-04,  4.66353697e-04, ...,\n",
       "          2.96043203e-04,  4.78859409e-04,  3.25036846e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "monthly-circle",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      multiple                  228019    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                multiple                  16941056  \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  24590049  \n",
      "=================================================================\n",
      "Total params: 75,321,748\n",
      "Trainable params: 75,321,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "moved-compound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "550/550 [==============================] - 563s 981ms/step - loss: 3.4055 - val_loss: 2.6118\n",
      "Epoch 2/10\n",
      "550/550 [==============================] - 540s 981ms/step - loss: 2.5399 - val_loss: 2.3950\n",
      "Epoch 3/10\n",
      "550/550 [==============================] - 540s 982ms/step - loss: 2.3420 - val_loss: 2.2842\n",
      "Epoch 4/10\n",
      "550/550 [==============================] - 540s 981ms/step - loss: 2.1995 - val_loss: 2.1852\n",
      "Epoch 5/10\n",
      "550/550 [==============================] - 539s 981ms/step - loss: 2.0609 - val_loss: 2.1111\n",
      "Epoch 6/10\n",
      "550/550 [==============================] - 540s 983ms/step - loss: 1.9326 - val_loss: 2.0475\n",
      "Epoch 7/10\n",
      "550/550 [==============================] - 540s 981ms/step - loss: 1.7906 - val_loss: 1.9964\n",
      "Epoch 8/10\n",
      "550/550 [==============================] - 540s 981ms/step - loss: 1.6534 - val_loss: 1.9508\n",
      "Epoch 9/10\n",
      "550/550 [==============================] - 540s 981ms/step - loss: 1.5199 - val_loss: 1.9112\n",
      "Epoch 10/10\n",
      "550/550 [==============================] - 540s 981ms/step - loss: 1.3892 - val_loss: 1.8852\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(enc_train, \n",
    "          dec_train, \n",
    "          epochs=epochs,\n",
    "          batch_size=256,\n",
    "          validation_data=(enc_val, dec_val),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-elements",
   "metadata": {},
   "source": [
    "평가문항 1번"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-script",
   "metadata": {},
   "source": [
    " ###  문장 만드는 과정.\n",
    " \n",
    "1. 입력받은 문장의 텐서를 입력합니다\n",
    "2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "4.모델이 <end>를 예측했거나, max_len에 도달했다면 \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "moderate-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    \n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "   \n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "     \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bibliographic-continuity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> i love the way you lie <end> \n"
     ]
    }
   ],
   "source": [
    "test_sen = generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)\n",
    "print(test_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-addition",
   "metadata": {},
   "source": [
    "프로젝트 회고\n",
    "\n",
    "* 처음에 토큰의 갯수를 어떻게 제거해야하는 아이디어를 몰라서 너무나 많이 애를 먹은 것 같습니다. 아직 조건문에 대한 이해도가 너무 낮은 것 같고 NLP는 결국 조건 설정이 가장 중요하다는 것을 알게되었습니다.\n",
    "\n",
    "\n",
    "\n",
    "* 문장의 길이가 15개 이하를 말했을 때 처음에는 start와 end를 포함하지 않는 것을 생각했으나 막상 프로그램을 돌리고 생각을 해보니 포함하여 15개인 것을 알게 되어 tokenize를 했을 때 13개 이하만 되도록 corpus를 만들었습니다.\n",
    "\n",
    "\n",
    "\n",
    "* batch_size를 처음에는 256에서 나중에는 2048까지 올렸습니다 왜냐하면 1epoch가 걸리는 시간이 배수까지는 아니더라도 그의 준하게 줄어드는지 아닌지가 궁금해서 입니다. 허나 막상 해보았을 때는 1.1배 정도 밖에 차이가 나지 않아서 너무 단순한 저의 뇌구조에 어이가 없었습니다.\n",
    "\n",
    "* val_loss를 맞추기 위해서 파라미터들을 다양하게 바꾸면서 최적의 loss을 찾는 법을 배우게 된 것 같습니다.\n",
    "  이를 통해서 하이퍼파라미터가 주는 변화를 눈으로 볼 수 있었고 그로 인해 생기는 소요시간의 변화, loss의 변화들이 너무나 유의미해서 좋았습니다\n",
    "\n",
    "* 이번 프로젝트의 핵심은 결국 데이터의 전처리와 셋 구성 과정에서 padding처리, 특수문자제거, 토큰화 등등을 제대로 수행하여 우리가 원하는 인공지능을 구현할 수 있는지에 대해서 입니다.  CV와 NLP기반 인공지능은 뭔가 진짜 사람의 생각을 구현하는 듯해서 신기하고 재미있었습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-security",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
